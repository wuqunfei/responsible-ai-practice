{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pet Insurance Claims SHAP Analysis Tutorial\n",
    "\n",
    "This notebook provides an interactive tutorial for using SHAP to explain GPT model decisions on pet insurance claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T16:22:44.149068Z",
     "start_time": "2025-11-09T16:22:42.179620Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.shap_explainer import PetClaimExplainer\n",
    "from src.semantic_analyzer import SemanticAnalyzer\n",
    "from src.visualization import ClaimVisualizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Setup complete!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T16:23:36.934331Z",
     "start_time": "2025-11-09T16:23:36.329278Z"
    }
   },
   "source": [
    "# Initialize explainer (using GPT-2 as example)\n",
    "explainer = PetClaimExplainer(model_name='gpt2')\n",
    "print(\"✓ SHAP Explainer initialized\")\n",
    "\n",
    "# Initialize semantic analyzer\n",
    "analyzer = SemanticAnalyzer()\n",
    "print(\"✓ Semantic Analyzer initialized\")\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = ClaimVisualizer()\n",
    "print(\"✓ Visualizer initialized\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SHAP Explainer initialized\n",
      "✓ Semantic Analyzer initialized\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/.venv/lib/python3.12/site-packages/matplotlib/style/core.py:129\u001B[39m, in \u001B[36muse\u001B[39m\u001B[34m(style)\u001B[39m\n\u001B[32m    128\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m129\u001B[39m     style = \u001B[43m_rc_params_in_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstyle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/.venv/lib/python3.12/site-packages/matplotlib/__init__.py:906\u001B[39m, in \u001B[36m_rc_params_in_file\u001B[39m\u001B[34m(fname, transform, fail_on_error)\u001B[39m\n\u001B[32m    905\u001B[39m rc_temp = {}\n\u001B[32m--> \u001B[39m\u001B[32m906\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_open_file_or_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfd\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    907\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mtry\u001B[39;49;00m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.local/share/uv/python/cpython-3.12.0-macos-aarch64-none/lib/python3.12/contextlib.py:137\u001B[39m, in \u001B[36m_GeneratorContextManager.__enter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    136\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m137\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    138\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/.venv/lib/python3.12/site-packages/matplotlib/__init__.py:883\u001B[39m, in \u001B[36m_open_file_or_url\u001B[39m\u001B[34m(fname)\u001B[39m\n\u001B[32m    882\u001B[39m fname = os.path.expanduser(fname)\n\u001B[32m--> \u001B[39m\u001B[32m883\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mutf-8\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m    884\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m f\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'seaborn'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m✓ Semantic Analyzer initialized\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# Initialize visualizer\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m visualizer = \u001B[43mClaimVisualizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m✓ Visualizer initialized\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/04-Shap-Transform/notebooks/../src/visualization.py:26\u001B[39m, in \u001B[36mClaimVisualizer.__init__\u001B[39m\u001B[34m(self, style)\u001B[39m\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, style: \u001B[38;5;28mstr\u001B[39m = \u001B[33m'\u001B[39m\u001B[33mseaborn\u001B[39m\u001B[33m'\u001B[39m):\n\u001B[32m     20\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     21\u001B[39m \u001B[33;03m    Initialize visualizer with style settings\u001B[39;00m\n\u001B[32m     22\u001B[39m \u001B[33;03m    \u001B[39;00m\n\u001B[32m     23\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m     24\u001B[39m \u001B[33;03m        style: Matplotlib style to use\u001B[39;00m\n\u001B[32m     25\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m     \u001B[43mplt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstyle\u001B[49m\u001B[43m.\u001B[49m\u001B[43muse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstyle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     27\u001B[39m     \u001B[38;5;28mself\u001B[39m.colors = {\n\u001B[32m     28\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mapprove\u001B[39m\u001B[33m'\u001B[39m: \u001B[33m'\u001B[39m\u001B[33m#2ECC71\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m     29\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mreject\u001B[39m\u001B[33m'\u001B[39m: \u001B[33m'\u001B[39m\u001B[33m#E74C3C\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     32\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mpreventive\u001B[39m\u001B[33m'\u001B[39m: \u001B[33m'\u001B[39m\u001B[33m#3498DB\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m     33\u001B[39m     }\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/.venv/lib/python3.12/site-packages/matplotlib/style/core.py:131\u001B[39m, in \u001B[36muse\u001B[39m\u001B[34m(style)\u001B[39m\n\u001B[32m    129\u001B[39m         style = _rc_params_in_file(style)\n\u001B[32m    130\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m--> \u001B[39m\u001B[32m131\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[32m    132\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstyle\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[33m is not a valid package style, path of style \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    133\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mfile, URL of style file, or library style name (library \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    134\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mstyles are listed in `style.available`)\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m    135\u001B[39m filtered = {}\n\u001B[32m    136\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m style:  \u001B[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001B[39;00m\n",
      "\u001B[31mOSError\u001B[39m: 'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single Claim Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T16:23:48.086128Z",
     "start_time": "2025-11-09T16:23:47.840884Z"
    }
   },
   "source": [
    "# Example claim\n",
    "claim_text = \"Emergency surgery required for my 3-year-old dog after eating chocolate. Severe poisoning symptoms. Total cost $3,500.\"\n",
    "\n",
    "# Generate explanation\n",
    "explanation = explainer.explain_claim(claim_text)\n",
    "\n",
    "# Display results\n",
    "print(f\"Claim: {claim_text}\")\n",
    "print(f\"\\nDecision: {explanation['prediction']}\")\n",
    "print(f\"Confidence: {explanation['confidence']:.1%}\")\n",
    "print(f\"\\nMost influential factors:\")\n",
    "for token, importance in explanation['influential_tokens'][:5]:\n",
    "    impact = \"positive\" if importance > 0 else \"negative\"\n",
    "    print(f\"  • '{token}': {impact} impact ({importance:.3f})\")"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m claim_text = \u001B[33m\"\u001B[39m\u001B[33mEmergency surgery required for my 3-year-old dog after eating chocolate. Severe poisoning symptoms. Total cost $3,500.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Generate explanation\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m explanation = \u001B[43mexplainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexplain_claim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclaim_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Display results\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mClaim: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclaim_text\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/04-Shap-Transform/notebooks/../src/shap_explainer.py:79\u001B[39m, in \u001B[36mPetClaimExplainer.explain_claim\u001B[39m\u001B[34m(self, claim_text)\u001B[39m\n\u001B[32m     69\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     70\u001B[39m \u001B[33;03mGenerate SHAP explanation for a single claim\u001B[39;00m\n\u001B[32m     71\u001B[39m \u001B[33;03m\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     76\u001B[39m \u001B[33;03m    Dictionary containing explanation details\u001B[39;00m\n\u001B[32m     77\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     78\u001B[39m \u001B[38;5;66;03m# Get SHAP values\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m79\u001B[39m shap_values = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mexplainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mclaim_text\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     81\u001B[39m \u001B[38;5;66;03m# Get prediction\u001B[39;00m\n\u001B[32m     82\u001B[39m inputs = \u001B[38;5;28mself\u001B[39m.tokenizer(\n\u001B[32m     83\u001B[39m     claim_text, \n\u001B[32m     84\u001B[39m     return_tensors=\u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m, \n\u001B[32m     85\u001B[39m     truncation=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     86\u001B[39m ).to(\u001B[38;5;28mself\u001B[39m.device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/.venv/lib/python3.12/site-packages/shap/explainers/_partition.py:173\u001B[39m, in \u001B[36mPartitionExplainer.__call__\u001B[39m\u001B[34m(self, max_evals, fixed_context, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001B[39m\n\u001B[32m    161\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\n\u001B[32m    162\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    163\u001B[39m     *args,\n\u001B[32m   (...)\u001B[39m\u001B[32m    170\u001B[39m     silent=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    171\u001B[39m ):\n\u001B[32m    172\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m173\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    175\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    176\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfixed_context\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfixed_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    177\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmain_effects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmain_effects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m        \u001B[49m\u001B[43merror_bounds\u001B[49m\u001B[43m=\u001B[49m\u001B[43merror_bounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    179\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    180\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    181\u001B[39m \u001B[43m        \u001B[49m\u001B[43msilent\u001B[49m\u001B[43m=\u001B[49m\u001B[43msilent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    182\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/.venv/lib/python3.12/site-packages/shap/explainers/_explainer.py:364\u001B[39m, in \u001B[36mExplainer.__call__\u001B[39m\u001B[34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001B[39m\n\u001B[32m    362\u001B[39m     feature_names = [[] \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(args))]\n\u001B[32m    363\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m row_args \u001B[38;5;129;01min\u001B[39;00m show_progress(\u001B[38;5;28mzip\u001B[39m(*args), num_rows, \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m + \u001B[33m\"\u001B[39m\u001B[33m explainer\u001B[39m\u001B[33m\"\u001B[39m, silent):\n\u001B[32m--> \u001B[39m\u001B[32m364\u001B[39m     row_result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mexplain_row\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    365\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43mrow_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    366\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    367\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmain_effects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmain_effects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    368\u001B[39m \u001B[43m        \u001B[49m\u001B[43merror_bounds\u001B[49m\u001B[43m=\u001B[49m\u001B[43merror_bounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    369\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    370\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    371\u001B[39m \u001B[43m        \u001B[49m\u001B[43msilent\u001B[49m\u001B[43m=\u001B[49m\u001B[43msilent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    372\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    373\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    374\u001B[39m     values.append(row_result.get(\u001B[33m\"\u001B[39m\u001B[33mvalues\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    375\u001B[39m     output_indices.append(row_result.get(\u001B[33m\"\u001B[39m\u001B[33moutput_indices\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/.venv/lib/python3.12/site-packages/shap/explainers/_partition.py:204\u001B[39m, in \u001B[36mPartitionExplainer.explain_row\u001B[39m\u001B[34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, fixed_context, *row_args)\u001B[39m\n\u001B[32m    202\u001B[39m \u001B[38;5;66;03m# if not fixed background or no base value assigned then compute base value for a row\u001B[39;00m\n\u001B[32m    203\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._curr_base_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.masker, \u001B[33m\"\u001B[39m\u001B[33mfixed_background\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m--> \u001B[39m\u001B[32m204\u001B[39m     \u001B[38;5;28mself\u001B[39m._curr_base_value = \u001B[43mfm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mm00\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mzero_index\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m[\n\u001B[32m    205\u001B[39m         \u001B[32m0\u001B[39m\n\u001B[32m    206\u001B[39m     ]  \u001B[38;5;66;03m# the zero index param tells the masked model what the baseline is\u001B[39;00m\n\u001B[32m    207\u001B[39m f11 = fm(~m00.reshape(\u001B[32m1\u001B[39m, -\u001B[32m1\u001B[39m))[\u001B[32m0\u001B[39m]\n\u001B[32m    209\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m.masker.clustering):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/.venv/lib/python3.12/site-packages/shap/utils/_masked_model.py:67\u001B[39m, in \u001B[36mMaskedModel.__call__\u001B[39m\u001B[34m(self, masks, zero_index, batch_size)\u001B[39m\n\u001B[32m     64\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._full_masking_call(full_masks, zero_index=zero_index, batch_size=batch_size)\n\u001B[32m     66\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_full_masking_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmasks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/.venv/lib/python3.12/site-packages/shap/utils/_masked_model.py:142\u001B[39m, in \u001B[36mMaskedModel._full_masking_call\u001B[39m\u001B[34m(self, masks, zero_index, batch_size)\u001B[39m\n\u001B[32m    139\u001B[39m         all_masked_inputs[i].append(v)\n\u001B[32m    141\u001B[39m joined_masked_inputs = \u001B[38;5;28mtuple\u001B[39m([np.concatenate(v) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m all_masked_inputs])\n\u001B[32m--> \u001B[39m\u001B[32m142\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mjoined_masked_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    143\u001B[39m _assert_output_input_match(joined_masked_inputs, outputs)\n\u001B[32m    144\u001B[39m all_outputs.append(outputs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/.venv/lib/python3.12/site-packages/shap/models/_model.py:23\u001B[39m, in \u001B[36mModel.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args):\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m     out = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minner_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     24\u001B[39m     is_tensor = safe_isinstance(out, \u001B[33m\"\u001B[39m\u001B[33mtorch.Tensor\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     25\u001B[39m     out = out.cpu().detach().numpy() \u001B[38;5;28;01mif\u001B[39;00m is_tensor \u001B[38;5;28;01melse\u001B[39;00m np.array(out)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/04-Shap-Transform/notebooks/../src/shap_explainer.py:47\u001B[39m, in \u001B[36mPetClaimExplainer._setup_explainer.<locals>.predict_proba\u001B[39m\u001B[34m(texts)\u001B[39m\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpredict_proba\u001B[39m(texts: List[\u001B[38;5;28mstr\u001B[39m]) -> np.ndarray:\n\u001B[32m     46\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Prediction function for SHAP\"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m     inputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     48\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     49\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     50\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     51\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     52\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m512\u001B[39;49m\n\u001B[32m     53\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m.to(\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m     55\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m     56\u001B[39m         outputs = \u001B[38;5;28mself\u001B[39m.model(**inputs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2938\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.__call__\u001B[39m\u001B[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[39m\n\u001B[32m   2936\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._in_target_context_manager:\n\u001B[32m   2937\u001B[39m         \u001B[38;5;28mself\u001B[39m._switch_to_input_mode()\n\u001B[32m-> \u001B[39m\u001B[32m2938\u001B[39m     encodings = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mall_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2939\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   2940\u001B[39m     \u001B[38;5;28mself\u001B[39m._switch_to_target_mode()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/responsible-ai-practice/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2998\u001B[39m, in \u001B[36mPreTrainedTokenizerBase._call_one\u001B[39m\u001B[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[39m\n\u001B[32m   2995\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m   2997\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_valid_text_input(text):\n\u001B[32m-> \u001B[39m\u001B[32m2998\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   2999\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mtext input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   3000\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mor `list[list[str]]` (batch of pretokenized examples).\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   3001\u001B[39m     )\n\u001B[32m   3003\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_valid_text_input(text_pair):\n\u001B[32m   3004\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   3005\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mtext input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   3006\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mor `list[list[str]]` (batch of pretokenized examples).\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   3007\u001B[39m     )\n",
      "\u001B[31mValueError\u001B[39m: text input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T16:23:53.886466Z",
     "start_time": "2025-11-09T16:23:53.868660Z"
    }
   },
   "source": [
    "# Visualize the explanation\n",
    "fig = visualizer.plot_single_explanation(claim_text, explanation)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'visualizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Visualize the explanation\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m fig = \u001B[43mvisualizer\u001B[49m.plot_single_explanation(claim_text, explanation)\n\u001B[32m      3\u001B[39m plt.show()\n",
      "\u001B[31mNameError\u001B[39m: name 'visualizer' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract semantic features\n",
    "features = analyzer.extract_features(claim_text)\n",
    "\n",
    "# Display features\n",
    "print(\"Semantic Features:\")\n",
    "for feature, value in features.items():\n",
    "    if isinstance(value, bool):\n",
    "        print(f\"  {feature}: {'Yes' if value else 'No'}\")\n",
    "    elif isinstance(value, (int, float)):\n",
    "        print(f\"  {feature}: {value:.2f}\" if isinstance(value, float) else f\"  {feature}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset\n",
    "sample_claims = [\n",
    "    \"Emergency surgery for dog who ate chocolate. Bill: $4000\",\n",
    "    \"Routine dental cleaning for cat. Cost: $300\",\n",
    "    \"Dog hit by car, multiple fractures. Emergency treatment $5500\",\n",
    "    \"Annual wellness checkup and vaccinations. $150\",\n",
    "    \"Cat diagnosed with diabetes, needs insulin. Monthly cost $200\",\n",
    "    \"Preventive flea and tick medication. $75\"\n",
    "]\n",
    "\n",
    "# Process all claims\n",
    "explanations = explainer.explain_batch(sample_claims)\n",
    "\n",
    "# Create results dataframe\n",
    "results = pd.DataFrame({\n",
    "    'claim': sample_claims,\n",
    "    'prediction': [exp['prediction'] for exp in explanations],\n",
    "    'confidence': [exp['confidence'] for exp in explanations]\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize batch summary\n",
    "fig = visualizer.plot_batch_summary(explanations, sample_claims)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pattern Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify patterns\n",
    "predictions = [exp['prediction'] for exp in explanations]\n",
    "patterns = analyzer.identify_patterns(sample_claims, predictions)\n",
    "\n",
    "# Display key differences\n",
    "print(\"Key Differences between Approved and Rejected Claims:\")\n",
    "for feature, diff in patterns['key_differences'].items():\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  Approved: {diff['approved_value']:.2f}\")\n",
    "    print(f\"  Rejected: {diff['rejected_value']:.2f}\")\n",
    "    print(f\"  Difference: {diff['difference']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate business rules\n",
    "rules = analyzer.generate_business_rules(patterns)\n",
    "\n",
    "print(\"Generated Business Rules:\")\n",
    "for i, rule in enumerate(rules, 1):\n",
    "    print(f\"{i}. {rule}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Claim Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_claim(claim_text):\n",
    "    \"\"\"Interactive function to test any claim\"\"\"\n",
    "    # Get explanation\n",
    "    explanation = explainer.explain_claim(claim_text)\n",
    "    features = analyzer.extract_features(claim_text)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLAIM: {claim_text}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nDECISION: {explanation['prediction']} (Confidence: {explanation['confidence']:.1%})\")\n",
    "    \n",
    "    print(\"\\nKEY FACTORS:\")\n",
    "    for token, imp in explanation['influential_tokens'][:3]:\n",
    "        arrow = \"↑\" if imp > 0 else \"↓\"\n",
    "        print(f\"  {arrow} {token} ({imp:+.3f})\")\n",
    "    \n",
    "    print(\"\\nSEMANTIC ANALYSIS:\")\n",
    "    print(f\"  Emergency: {'Yes' if features['is_emergency'] else 'No'}\")\n",
    "    print(f\"  Preventive: {'Yes' if features['is_preventive'] else 'No'}\")\n",
    "    print(f\"  Severity Score: {features['severity_score']:.2f}\")\n",
    "    if features['cost_amount']:\n",
    "        print(f\"  Cost: ${features['cost_amount']:,.0f}\")\n",
    "\n",
    "# Test your own claims\n",
    "test_claim(\"My puppy needs emergency stomach surgery after swallowing a toy. Vet bill is $2800.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another example\n",
    "test_claim(\"Annual teeth cleaning and nail trim for my healthy 5-year-old cat. Cost $125.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Analysis: Decision Rules Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract decision rules from multiple explanations\n",
    "decision_rules = explainer.get_decision_rules(explanations)\n",
    "\n",
    "print(\"Approval Indicators (with >10% support):\")\n",
    "for token, stats in sorted(decision_rules['approval_indicators'].items(), \n",
    "                         key=lambda x: x[1]['avg_importance'], reverse=True)[:5]:\n",
    "    print(f\"  '{token}': avg importance = {stats['avg_importance']:.3f}, \"\n",
    "          f\"frequency = {stats['frequency']}, support = {stats['support']:.1%}\")\n",
    "\n",
    "print(\"\\nRejection Indicators (with >10% support):\")\n",
    "for token, stats in sorted(decision_rules['rejection_indicators'].items(), \n",
    "                         key=lambda x: x[1]['avg_importance'])[:5]:\n",
    "    print(f\"  '{token}': avg importance = {stats['avg_importance']:.3f}, \"\n",
    "          f\"frequency = {stats['frequency']}, support = {stats['support']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "results_detailed = results.copy()\n",
    "\n",
    "# Add semantic features\n",
    "for i, claim in enumerate(sample_claims):\n",
    "    features = analyzer.extract_features(claim)\n",
    "    results_detailed.loc[i, 'is_emergency'] = features['is_emergency']\n",
    "    results_detailed.loc[i, 'is_preventive'] = features['is_preventive']\n",
    "    results_detailed.loc[i, 'severity_score'] = features['severity_score']\n",
    "    results_detailed.loc[i, 'cost_amount'] = features['cost_amount']\n",
    "\n",
    "# Save to CSV\n",
    "results_detailed.to_csv('claim_analysis_results.csv', index=False)\n",
    "print(\"Results saved to 'claim_analysis_results.csv'\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(results_detailed.groupby('prediction')[['confidence', 'severity_score']].agg(['mean', 'std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "1. **Fine-tune the model**: Train on your actual claims data\n",
    "2. **Customize semantic features**: Add domain-specific patterns\n",
    "3. **Deploy to production**: Integrate with your claims system\n",
    "4. **Monitor and improve**: Track accuracy and update rules\n",
    "\n",
    "For more information, see the project README and documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
